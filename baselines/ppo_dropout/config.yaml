defaults:
  exp_name: ppo_dropout_baseline
  seed: 0
  torch_deterministic: true
  cuda: true
  track: true
  wandb_project: "baseline-712"
  wandb_entity: null
  capture_video: false

  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000
  num_envs: 8
  use_wandb: true
  masking_strategy: cycle  # Strategy for selecting sensor subsets: cycle, random, or adaptive

  # Adaptive scheduling configuration
  adaptive:
    warmup_timesteps: 50000  # Use uniform sampling for first 50k timesteps before switching to adaptive
    smoothing_factor: 0.1    # EMA smoothing factor: 0.1 = slow, 0.9 = fast, null/0 = no smoothing (latest only), -1 = simple average
    min_evaluations: 3       # Minimum number of evaluations before using adaptive probabilities (1 = immediate)

  # Teacher and student observation keys
  keys: {mlp_keys: '.*', cnn_keys: '.*'}
  num_eval_configs: 8
  eval_keys:
    env1:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env3:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env4:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env5:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env6:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env7:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env8:
      mlp_keys: '.*'
      cnn_keys: '.*'

  save_model: false
  log_keys_video: [image]

  # Environment settings
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: False, actions: all, lives: unused, noops: 0, resize: opencv}
    dmlab: {size: [64, 64], repeat: 4, episodic: True}
    minecraft: {size: [64, 64], break_speed: 100.0}
    dmc: {size: [64, 64], repeat: 2, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}
    gym: {obs_key: state}
    robopianist: {render_image: true}
  
  # PPO Hyperparameters (Stable Baselines 3 defaults)
  ppo:
    learning_rate: 3e-4      # Default SB3 learning rate
    n_steps: 2048           # Number of steps to run for each environment per update
    batch_size: 64          # Minibatch size
    n_epochs: 10            # Number of epoch when optimizing the surrogate loss
    gamma: 0.99             # Discount factor
    gae_lambda: 0.95        # Factor for trade-off of bias vs variance for GAE
    clip_range: 0.2         # Clipping parameter for PPO
    clip_range_vf: null     # Clipping parameter for value function (None = no clipping)
    ent_coef: 0.0           # Entropy coefficient for the loss calculation
    vf_coef: 0.5            # Value function coefficient for the loss calculation
    max_grad_norm: 0.5      # The maximum value for the gradient clipping
    use_sde: false          # Whether to use generalized State Dependent Exploration (gSDE)
    sde_sample_freq: -1     # Sample a new noise matrix every n steps when using gSDE
    target_kl: null         # Limit the KL divergence between updates (None = no limit)
    policy_kwargs: {}       # Additional arguments for the policy
  
  # Evaluation settings
  eval:
    eval_freq: 2048  # Evaluate every 2048 callback calls (matches rollout frequency)
    n_eval_episodes: 10  # Reduced episodes since we're evaluating much more frequently
    eval_envs: 4  # Number of parallel environments for evaluation
    video_log_interval: 1  # Log videos every evaluation call
    num_eval_configs: 4
  
  # Training settings
  log_interval: 1  # Log every rollout

# ====================
# Named Configs
# ====================

# Tiger Door Key
gymnasium_tigerdoorkey: &tigerdoorkey_base
  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  # Reduced learning rate for stability in simple grid environment
  ppo:
    learning_rate: 1e-4      # Reduced from 3e-4 for more stable learning
  # Adaptive scheduling configuration for TigerDoorKey
  adaptive:
    warmup_timesteps: 20000  # Shorter warmup for simpler environment
    smoothing_factor: 0.2    # Slightly faster adaptation
    min_evaluations: 2       # Fewer evaluations needed
  num_eval_configs: 4
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env2: # must press button to reveal tiger/treasure up-down relative position. starts with key
      mlp_keys: '\b(neighbors_unprivileged_nokey|door_unprivileged|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env3:  # must get key to reveal unlocked doors. starts with button
      mlp_keys: '\b(neighbors_unprivileged_nobutton|door|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

    env4:  # must press button and get key to reveal unlocked doors and tiger/treasure relative position. starts with nothing
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

  exp_name: "tigerdoorkey_ppo_dropout"



# Tiger Door Key
debug_gymnasium_tigerdoorkey: &debug_tigerdoorkey_base
  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  # Reduced learning rate for stability in simple grid environment
  ppo:
    learning_rate: 1e-4      # Reduced from 3e-4 for more stable learning
  # Example: No smoothing (backward compatible with old behavior)
  adaptive:
    warmup_timesteps: 10000  # Short warmup for debugging
    smoothing_factor: null   # No smoothing - use latest evaluation only
    min_evaluations: 1       # Immediate adaptation after first evaluation
  num_eval_configs: 2
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'
    env2:  # must press button and get key to reveal unlocked doors and tiger/treasure relative position. starts with nothing
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

  exp_name: "adaptive_debug_ppo_dropout"



gymnasium_tigerdoorkeylarge:
  << : *tigerdoorkey_base
  task: "gymnasium_tigerdoorkeylarge"
  total_timesteps: 500000
  exp_name: "tigerdoorkeylarge_ppo_dropout"

gymnasium_maze: &maze_base
  task: gymnasium_Maze7x7-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b', cnn_keys: '^$'}  # omit the distance key
  # Adaptive scheduling configuration for Maze (more conservative due to complexity)
  adaptive:
    warmup_timesteps: 80000  # Longer warmup for more complex environment
    smoothing_factor: 0.1    # Slower adaptation to avoid noise
    min_evaluations: 4       # More evaluations needed for stable estimates
  num_eval_configs: 6
  eval_keys:  # Note that neighbors_5x5 only includes neighbors not in the 3x3 neighbors (they are disjoint)
    env1:  # Goal position, neighbors 5x5
      mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    
    env2:  # Goal position, neighbors_3x3
      mlp_keys: '\b(goal_position|position|neighbors_3x3)\b'
      cnn_keys: '^$'
    
    env3:  # No goal position, neighbors_3x3
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3)\b'
      cnn_keys: '^$'

    env4:  # No goal position, neighbors_5x5
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    
    env5: # No goal position, neighbors_3x3 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '^$'
    
    env6: # No goal position, neighbors_5x5 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged|neighbors_5x5_unprivileged)\b'
      cnn_keys: '^$'

  exp_name: "maze_ppo_dropout"

gymnasium_maze11:
  << : *maze_base
  task: gymnasium_Maze11x11-v0
  total_timesteps: 1500000
  exp_name: "maze11_ppo_dropout"

# Blind Pick environment
gymnasium_blindpick: &blindpick_base
  task: gymnasium_FOFixedGripper2DBlind7cmPick-v0
  total_timesteps: 5000000
  num_envs: 32
  log_keys_video: [camera_front]
  keys: {mlp_keys: '.*', cnn_keys: '.*'} # robot_state, touch, camera_front, camera_side, gripper_camera_rgb
  num_eval_configs: 7
  eval_keys:
    env1: # full privileged observations -- reach directly for object
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:  # NO TOUCH -- reach directly without touch
      mlp_keys: '\b(robot_state|obj_state)\b'
      cnn_keys: '.*'
    env3:  # NO OBJ STATE -- reach directly using image and not obj state
      mlp_keys: '\b(robot_state|touch)\b'
      cnn_keys: '.*'
    env4: # NO OBJ STATE, only a wrist camera -- searching behavior
      mlp_keys: '\b(robot_state|touch)\b'
      cnn_keys: 'gripper_camera_rgb'
    env5:  # full state, only wrist camera -- searching but obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'gripper_camera_rgb'
    env6:  # full state, only front camera -- searching but image, obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'camera_front'
    env7:  # full state, only side camera -- searching but image, obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'camera_side'

  exp_name: "blindpick_ppo_dropout"

# Blind Pick with different sensor configurations
blindpick_oracle_state:
  << : *blindpick_base
  keys: {mlp_keys: r'\b(robot_state|obj_state)\b', cnn_keys: '^$'}
  eval_keys:
    env1: # oracle state (robot_state + obj_state)
      mlp_keys: r'\b(robot_state|obj_state)\b'
      cnn_keys: '^$'
  exp_name: "blindpick_oracle_state_ppo_dropout"

blindpick_vision_only:
  << : *blindpick_base
  keys: {mlp_keys: '^$', cnn_keys: '.*'}
  eval_keys:
    env1: # vision only
      mlp_keys: '^$'
      cnn_keys: '.*'
  exp_name: "blindpick_vision_only_ppo_dropout"

blindpick_touch_vision:
  << : *blindpick_base
  keys: {mlp_keys: 'touch', cnn_keys: '.*'}
  eval_keys:
    env1: # touch + vision
      mlp_keys: 'touch'
      cnn_keys: '.*'
  exp_name: "blindpick_touch_vision_ppo_dropout"

blindpick_robot_state_vision:
  << : *blindpick_base
  keys: {mlp_keys: 'robot_state', cnn_keys: '.*'}
  eval_keys:
    env1: # robot state + vision
      mlp_keys: 'robot_state'
      cnn_keys: '.*'
  exp_name: "blindpick_robot_state_vision_ppo_dropout"

# ====================
# Adaptive Smoothing Mode Examples
# ====================

# Example: Latest value only (no smoothing, backward compatible)
gymnasium_tigerdoorkey_latest:
  << : *tigerdoorkey_base
  adaptive:
    warmup_timesteps: 10000
    smoothing_factor: null   # or 0 - no smoothing
    min_evaluations: 1       # immediate adaptation
  exp_name: "tigerdoorkey_latest_ppo_dropout"

# Example: Simple average (cumulative mean)
gymnasium_tigerdoorkey_average:
  << : *tigerdoorkey_base
  adaptive:
    warmup_timesteps: 20000
    smoothing_factor: -1     # simple average of all evaluations
    min_evaluations: 3       # wait for more stable average
  exp_name: "tigerdoorkey_average_ppo_dropout"

# Example: Fast EMA adaptation
gymnasium_tigerdoorkey_fast_ema:
  << : *tigerdoorkey_base
  adaptive:
    warmup_timesteps: 15000
    smoothing_factor: 0.5    # fast adaptation to recent performance
    min_evaluations: 2
  exp_name: "tigerdoorkey_fast_ema_ppo_dropout" 