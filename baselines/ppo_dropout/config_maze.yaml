defaults:
  exp_name: ppo_dropout_baseline
  seed: 0
  torch_deterministic: true
  cuda: true
  track: true
  wandb_project: "baseline-712"
  wandb_entity: null
  capture_video: false

  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 400_000
  num_envs: 8
  use_wandb: true
  masking_strategy: cycle  # Strategy for selecting sensor subsets: cycle, uniform, or adaptive

  # Adaptive scheduling configuration
  adaptive:
    warmup_timesteps: 50000  # Use uniform sampling for first 50k timesteps before switching to adaptive
    evaluation_window: 3     # Number of most recent evaluations to keep in window
    smoothing_factor: 0.0      # 0 = average the window, >0 = exponential moving average over window

  # Teacher and student observation keys
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  num_eval_configs: 2
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'
    env2:
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

  save_model: false
  log_keys_video: [image]

  # Environment settings
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: False, actions: all, lives: unused, noops: 0, resize: opencv}
    dmlab: {size: [64, 64], repeat: 4, episodic: True}
    minecraft: {size: [64, 64], break_speed: 100.0}
    dmc: {size: [64, 64], repeat: 2, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}
    gym: {obs_key: state}
    robopianist: {render_image: true}
  
  # PPO Hyperparameters (Stable Baselines 3 defaults)
  ppo:
    learning_rate: 3e-4      # Reduced from 3e-4 for more stable learning in TigerDoorKey
    n_steps: 2048           # Number of steps to run for each environment per update
    batch_size: 64          # Minibatch size
    n_epochs: 10            # Number of epoch when optimizing the surrogate loss
    gamma: 0.99             # Discount factor
    gae_lambda: 0.95        # Factor for trade-off of bias vs variance for GAE
    clip_range: 0.2         # Clipping parameter for PPO
    clip_range_vf: null     # Clipping parameter for value function (None = no clipping)
    ent_coef: 0.0           # Entropy coefficient for the loss calculation
    vf_coef: 0.5            # Value function coefficient for the loss calculation
    max_grad_norm: 0.5      # The maximum value for the gradient clipping
    use_sde: false          # Whether to use generalized State Dependent Exploration (gSDE)
    sde_sample_freq: -1     # Sample a new noise matrix every n steps when using gSDE
    target_kl: null         # Limit the KL divergence between updates (None = no limit)
    policy_kwargs: {}       # Additional arguments for the policy
  
  # Evaluation settings
  eval:
    eval_freq: 2048  # Evaluate every 2048 callback calls (matches rollout frequency)
    n_eval_episodes: 10  # Reduced episodes since we're evaluating much more frequently
    eval_envs: 4
    video_log_interval: 1  # Log videos every evaluation call
    num_eval_configs: 4
  
  # Training settings
  log_interval: 1  # Log every rollout

# ====================
# Debug Configs
# ====================

# Base debug configuration for Maze
debug_maze_base: &debug_maze_base
  task: gymnasium_Maze7x7-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b', cnn_keys: '^$'}  # omit the distance key
  num_eval_configs: 2
  eval_keys:  # Note that neighbors_5x5 only includes neighbors not in the 3x3 neighbors (they are disjoint)
    env1:  # Goal position, neighbors 5x5
      mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'

    env2: # No goal position, neighbors_5x5 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_5x5_unprivileged)\b'
      cnn_keys: '^$'

# Debug Maze with Cycle strategy (deterministic cycling)
debug_maze_cycle:
  <<: *debug_maze_base
  masking_strategy: cycle
  exp_name: "debug_maze_cycle"

# Debug Maze with Adaptive strategy (no warmup, immediate adaptation)
debug_maze_adaptive_window1:
  <<: *debug_maze_base
  masking_strategy: adaptive
  adaptive:
    warmup_timesteps: 0      # No warmup - immediate adaptive behavior
    evaluation_window: 1     # Use only the most recent evaluation
    smoothing_factor: 0      # Average the window
  exp_name: "debug_maze_adaptive_naive"

# # Debug Maze with warmup
# debug_maze_adaptive_warmup_window1:
#   <<: *debug_maze_base
#   masking_strategy: adaptive
#   adaptive:
#     warmup_timesteps: 100_000  # 100k timesteps warmup (uses cycling during warmup)
#     evaluation_window: 1     # Use only the most recent evaluation
#     smoothing_factor: 0      # Average the window
#   exp_name: "debug_maze_adaptive_warmup"

# # Debug Maze with adaptive (100k warmup, window 3 average)
# debug_maze_adaptive_warmup_window3:
#   <<: *debug_maze_base
#   masking_strategy: adaptive
#   adaptive:
#     warmup_timesteps: 100_000  # 100k warmup then adapt
#     evaluation_window: 3       # Look at last 3 evals
#     smoothing_factor: 0        # Average window
#   exp_name: "debug_maze_adaptive_warmup_window3"

# debug_maze_adaptive_warmup_window100:
#   <<: *debug_maze_base
#   masking_strategy: adaptive
#   adaptive:
#     warmup_timesteps: 100_000  # 100k warmup then adapt
#     evaluation_window: 100       # Look at last 100 evals
#     smoothing_factor: 0        # Average window
#   exp_name: "debug_maze_adaptive_warmup_window100"

# # Debug Maze with adaptive (100k warmup, window 100 EMA)
# debug_maze_adaptive_warmup_window100_ema:
#   <<: *debug_maze_base
#   masking_strategy: adaptive
#   adaptive:
#     warmup_timesteps: 100_000
#     evaluation_window: 100
#     smoothing_factor: 0.5      # Exponential smoothing
#   exp_name: "debug_maze_adaptive_warmup_window100_ema"