defaults:
  exp_name: ppo_baseline
  seed: 0
  torch_deterministic: true
  cuda: true
  track: true
  wandb_project_name: "sensor-dropout"
  wandb_entity: null
  capture_video: false

  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000
  learning_rate: 3e-4
  num_envs: 8
  num_steps: 2048
  anneal_lr: true
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 4
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null
  eps: 1e-5
  batch_size: 8192
  minibatch_size: 2048

  # Teacher and student observation keys
  full_keys: {mlp_keys: '.*', cnn_keys: '.*'}
  keys: {mlp_keys: '.*', cnn_keys: '.*'}  # Only one set of keys for now
  eval_keys:
    env1:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env3:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env4:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env5:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env6:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env7:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env8:
      mlp_keys: '.*'
      cnn_keys: '.*'

  save_model: false
  log_keys_video: [image]

  # Logging
  log_interval: 10
  use_wandb: true
  wandb_project: sensor-dropout

  # PPO Encoder Architecture
  encoder:
    act: "silu"  # Activation function
    norm: "layer"  # Normalization type
    output_dim: 512  # Final output dimension
    mlp_layers: 2  # Number of MLP layers
    mlp_units: 256  # Number of units per MLP layer
    cnn_depth: 48  # Base depth for CNN
    cnn_blocks: 0  # Number of residual blocks
    resize: "bilinear"  # Resize strategy
    minres: 4  # Minimum resolution
  
  # Actor and critic architecture
  actor_critic:
    act: "silu"  # Activation function
    norm: "layer"  # Normalization type

  # Environment settings
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: False, actions: all, lives: unused, noops: 0, resize: opencv}
    dmlab: {size: [64, 64], repeat: 4, episodic: True}
    minecraft: {size: [64, 64], break_speed: 100.0}
    dmc: {size: [64, 64], repeat: 2, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}
    gym: {obs_key: state}
    robopianist: {render_image: true}
  
  # Evaluation settings
  eval:
    eval_interval: 1000  # Evaluate every 1000 steps
    num_eval_episodes: 10  # Number of episodes to evaluate
    eval_envs: 4  # Number of parallel environments for evaluation
    video_log_interval: 1  # Log videos every evaluation call
    num_eval_configs: 4

# ====================
# Named Configs
# ====================

# Tiger Key Door
gymnasium_tigerkeydoor: &tigerkeydoor_base
  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000 # 150000
  num_envs: 8
  num_steps: 256
  learning_rate: 1e-4
  ent_coef: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 8
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  eval:
    eval_interval: 100
    num_eval_episodes: 10
    eval_envs: 4
    num_eval_configs: 4
  full_keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env2: # must press button to reveal tiger/treasure up-down relative position
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env3:  # must get key to reveal unlocked doors
      mlp_keys: '\b(neighbors_unprivileged|door|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

    env4:  # must press button and get key to reveal unlocked doors and tiger/treasure relative position
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

  exp_name: "tigerkeydoor_ppo"

gymnasium_tigerkeydoorlarge:
  << : *tigerkeydoor_base
  task: gymnasium_TigerDoorKeyLarge-v0
  total_timesteps: 500000
  exp_name: "tigerkeydoorlarge_ppo"

gymnasium_maze: &maze_base
  task: gymnasium_Maze7x7-v0
  total_timesteps: 150000
  num_envs: 8
  num_steps: 256
  learning_rate: 1e-4
  ent_coef: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 8
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  eval:
    eval_interval: 100
    num_eval_episodes: 10
    eval_envs: 4
    num_eval_configs: 6  # sets of eval keys
  
  full_keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3)\b', cnn_keys: '\b(neighbors_5x5)\b'}  # omit the distance key
  keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3)\b', cnn_keys: '\b(neighbors_5x5)\b'}
  eval_keys:
    env1:  # Full privileged observations
      mlp_keys: '\b(goal_position|position|neighbors_3x3)\b'
      cnn_keys: '\b(neighbors_5x5)\b'
    
    env2:  # Only using neighbors_3x3
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3)\b'
      cnn_keys: '\b(neighbors_5x5_unprivileged)\b'

    env3:  # Only using neighbors_5x5
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '\b(neighbors_5x5)\b'

    env4:  # Use neighbors 3x3 and goal position
      mlp_keys: '\b(goal_position|position|neighbors_3x3)\b'
      cnn_keys: '\b(neighbors_5x5_unprivileged)\b'
    
    env5: # Use neighbors 5x5 and goal position
      mlp_keys: '\b(goal_position|position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '\b(neighbors_5x5)\b'
    
    env6: # Only using both sets of neighbors, no goal position
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3)\b'
      cnn_keys: '\b(neighbors_5x5)\b'

  exp_name: "maze_ppo"

# Blind Pick environment
gymnasium_blindpick: &blindpick_base
  task: gymnasium_FOFixedGripper2DBlind7cmPick-v0
  total_timesteps: 200000
  num_envs: 4
  num_steps: 128
  log_keys_video: [camera_front]
  eval:
    eval_interval: 1000
    num_eval_episodes: 10
    eval_envs: 4
    video_log_interval: 1
    num_eval_configs: 7
  full_keys: {mlp_keys: '.*', cnn_keys: '.*'} # robot_state, touch, camera_front, camera_side, gripper_camera_rgb
  keys: {mlp_keys: '.*', cnn_keys: '^$'}
  eval_keys:
    env1: # full privileged observations
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:  # robot state, NO touch, but using all cameras
      mlp_keys: 'robot_state'
      cnn_keys: '.*'
    env3:  # NO robot state, touch, but using all cameras
      mlp_keys: 'touch'
      cnn_keys: '.*'
    env4: # NO robot state, NO touch, ONLY all cameras
      mlp_keys: '^$'
      cnn_keys: '.*'
    env5:  # robot state, touch, but only wrist camera
      mlp_keys: '.*'
      cnn_keys: 'gripper_camera_rgb'
    env6:  # no robot state, touch, but only front camera
      mlp_keys: '.*'
      cnn_keys: 'camera_front'
    env7:  # robot state, touch, but only side camera
      mlp_keys: '.*'
      cnn_keys: 'camera_side'

  exp_name: "blindpick_ppo"

# Logging configurations
no_wandb:
  use_wandb: false
  exp_name: "ppo_no_wandb"

wandb_debug:
  use_wandb: true
  wandb_project: "sensor-dropout-debug"
  exp_name: "ppo_debug"

# Network architecture variations
large_network:
  encoder:
    output_dim: 1024
    mlp_units: 512
    cnn_depth: 64
  exp_name: "ppo_large_network"

small_network:
  encoder:
    output_dim: 256
    mlp_units: 128
    cnn_depth: 32
  exp_name: "ppo_small_network"

# Training variations
high_entropy:
  ent_coef: 0.1
  exp_name: "ppo_high_entropy"

low_entropy:
  ent_coef: 0.001
  exp_name: "ppo_low_entropy"

fast_learning:
  learning_rate: 1e-3
  num_steps: 64
  exp_name: "ppo_fast_learning"

slow_learning:
  learning_rate: 1e-4
  num_steps: 4096
  exp_name: "ppo_slow_learning" 