defaults:
  exp_name: ppo_lstm_baseline
  seed: 0
  torch_deterministic: true
  cuda: true
  track: true
  wandb_project_name: "sensor-dropout"
  wandb_entity: null
  capture_video: false

  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000
  learning_rate: 3e-4
  num_envs: 8
  num_steps: 2048
  anneal_lr: true
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 4
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null
  eps: 1e-5
  batch_size: 8192
  minibatch_size: 2048

  # LSTM settings
  lstm:
    hidden_size: 128  # LSTM hidden state size
    num_layers: 1     # Number of LSTM layers

  # Teacher and student observation keys
  full_keys: {mlp_keys: '.*', cnn_keys: '.*'}
  keys: {mlp_keys: '.*', cnn_keys: '.*'}  # Only one set of keys for now
  eval_keys:
    env1:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env3:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env4:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env5:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env6:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env7:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env8:
      mlp_keys: '.*'
      cnn_keys: '.*'

  save_model: false
  log_keys_video: [image]

  # Logging
  log_interval: 10
  use_wandb: true
  wandb_project: sensor-dropout

  # PPO Encoder Architecture
  encoder:
    act: "silu"  # Activation function
    norm: "layer"  # Normalization type
    output_dim: 512  # Final output dimension
    mlp_layers: 2  # Number of MLP layers
    mlp_units: 256  # Number of units per MLP layer
    cnn_depth: 48  # Base depth for CNN
    cnn_blocks: 0  # Number of residual blocks
    resize: "bilinear"  # Resize strategy
    minres: 4  # Minimum resolution
  
  # Actor and critic architecture
  actor_critic:
    act: "silu"  # Activation function
    norm: "layer"  # Normalization type

  # Environment settings
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: False, actions: all, lives: unused, noops: 0, resize: opencv}
    dmlab: {size: [64, 64], repeat: 4, episodic: True}
    minecraft: {size: [64, 64], break_speed: 100.0}
    dmc: {size: [64, 64], repeat: 2, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}
    gym: {obs_key: state}
    robopianist: {render_image: true}
  
  # Evaluation settings
  eval:
    eval_interval: 1000  # Evaluate every 1000 steps
    num_eval_episodes: 10  # Number of episodes to evaluate
    eval_envs: 4  # Number of parallel environments for evaluation
    video_log_interval: 1  # Log videos every evaluation call
    num_eval_configs: 4

# ====================
# Named Configs
# ====================

# Tiger Door Key
gymnasium_tigerdoorkey: &tigerdoorkey_base
  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000 # 150000
  num_envs: 8
  num_steps: 256
  learning_rate: 1e-4
  ent_coef: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 8
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  lstm:
    hidden_size: 128
    num_layers: 1
  eval:
    eval_interval: 100
    num_eval_episodes: 10
    eval_envs: 4
    num_eval_configs: 4
  full_keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env2: # must press button to reveal tiger/treasure up-down relative position
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'

    env3:  # must get key to reveal unlocked doors
      mlp_keys: '\b(neighbors_unprivileged|door|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

    env4:  # must press button and get key to reveal unlocked doors and tiger/treasure relative position
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'

  exp_name: "tigerdoorkey_ppo_lstm"

gymnasium_tigerdoorkeylarge:
  << : *tigerdoorkey_base
  task: "gymnasium_tigerdoorkeylarge"
  total_timesteps: 500000
  exp_name: "tigerdoorkeylarge_ppo_lstm"

gymnasium_maze: &maze_base
  task: gymnasium_Maze7x7-v0
  total_timesteps: 500000
  num_envs: 8
  num_steps: 256
  learning_rate: 1e-4
  ent_coef: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 8
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  lstm:
    hidden_size: 128
    num_layers: 1
  eval:
    eval_interval: 100
    num_eval_episodes: 10
    eval_envs: 4
    num_eval_configs: 6  # sets of eval keys
  
  full_keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b', cnn_keys: '^$'}  # omit the distance key
  keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b', cnn_keys: '^$'}
  eval_keys:  # Note that neighbors_5x5 only includes neighbors not in the 3x3 neighbors (they are disjoint)
    env1:  # Goal position, neighbors 5x5
      mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    
    env2:  # Goal position, neighbors_3x3
      mlp_keys: '\b(goal_position|position|neighbors_3x3)\b'
      cnn_keys: '^$'
    
    env3:  # No goal position, neighbors_3x3
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3)\b'
      cnn_keys: '^$'

    env4:  # No goal position, neighbors 5x5
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    
    env5: # No goal position, neighbors_3x3 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '^$'
    
    env6: # No goal position, neighbors_5x5 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged|neighbors_5x5_unprivileged)\b'
      cnn_keys: '^$'

  exp_name: "maze_ppo_lstm"

gymnasium_blindpick: &blindpick_base
  task: gymnasium_GridBlindPick-v0
  total_timesteps: 150000
  num_envs: 8
  num_steps: 256
  learning_rate: 1e-4
  ent_coef: 0.05
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 8
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: true
  vf_coef: 0.5
  max_grad_norm: 0.5
  lstm:
    hidden_size: 128
    num_layers: 1
  eval:
    eval_interval: 100
    num_eval_episodes: 10
    eval_envs: 4
    num_eval_configs: 4
  
  full_keys: {mlp_keys: '\b(position|goal_position|neighbors_3x3)\b', cnn_keys: '^$'}
  keys: {mlp_keys: '\b(position|goal_position|neighbors_3x3)\b', cnn_keys: '^$'}
  eval_keys:
    env1:  # Full observation
      mlp_keys: '\b(position|goal_position|neighbors_3x3)\b'
      cnn_keys: '^$'
    
    env2:  # No goal position
      mlp_keys: '\b(position|goal_position_unprivileged|neighbors_3x3)\b'
      cnn_keys: '^$'
    
    env3:  # No neighbors
      mlp_keys: '\b(position|goal_position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '^$'
    
    env4:  # No goal position and no neighbors
      mlp_keys: '\b(position|goal_position_unprivileged|neighbors_3x3_unprivileged)\b'
      cnn_keys: '^$'

  exp_name: "blindpick_ppo_lstm" 