defaults:
  exp_name: ppo_dropout_any_baseline
  seed: 0
  torch_deterministic: true
  cuda: true
  track: true
  wandb_project: "baseline-712"
  wandb_entity: null
  capture_video: false

  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 500000
  num_envs: 8
  use_wandb: true

  # Teacher and student observation keys
  keys: {mlp_keys: '.*', cnn_keys: '.*'}
  
  # Evaluation keys (for subset evaluation comparison with original PPO dropout)
  eval_keys:
    env1:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env3:
      mlp_keys: '.*'
      cnn_keys: '.*'
    env4:
      mlp_keys: '.*'
      cnn_keys: '.*'
  
  # Probabilistic dropout configuration (modular design)
  dropout:
    schedule_type: 'constant'  # Options: 'constant', 'linear', 'exponential'
    base_probability: 0.5      # For constant schedule
    # For linear schedule:
    # start_probability: 0.8
    # end_probability: 0.2
    # total_episodes: 10000
    # For exponential schedule:
    # start_probability: 0.8
    # decay_rate: 0.995
    # min_probability: 0.1

  save_model: false
  log_keys_video: [image]

  # Environment settings
  env:
    atari: {size: [64, 64], repeat: 4, sticky: True, gray: False, actions: all, lives: unused, noops: 0, resize: opencv}
    dmlab: {size: [64, 64], repeat: 4, episodic: True}
    minecraft: {size: [64, 64], break_speed: 100.0}
    dmc: {size: [64, 64], repeat: 2, camera: -1}
    loconav: {size: [64, 64], repeat: 2, camera: -1}
    gym: {obs_key: state}
    robopianist: {render_image: true}
  
  # PPO Hyperparameters (Stable Baselines 3 defaults)
  ppo:
    learning_rate: 3e-4      # Default SB3 learning rate
    n_steps: 2048           # Number of steps to run for each environment per update
    batch_size: 64          # Minibatch size
    n_epochs: 10            # Number of epoch when optimizing the surrogate loss
    gamma: 0.99             # Discount factor
    gae_lambda: 0.95        # Factor for trade-off of bias vs variance for GAE
    clip_range: 0.2         # Clipping parameter for PPO
    clip_range_vf: null     # Clipping parameter for value function (None = no clipping)
    ent_coef: 0.0           # Entropy coefficient for the loss calculation
    vf_coef: 0.5            # Value function coefficient for the loss calculation
    max_grad_norm: 0.5      # The maximum value for the gradient clipping
    use_sde: false          # Whether to use generalized State Dependent Exploration (gSDE)
    sde_sample_freq: -1     # Sample a new noise matrix every n steps when using gSDE
    target_kl: null         # Limit the KL divergence between updates (None = no limit)
    policy_kwargs: {}       # Additional arguments for the policy
  
  # Evaluation settings
  eval:
    eval_freq: 2048  # Evaluate every 2048 callback calls (matches rollout frequency)
    n_eval_episodes: 10  # Number of evaluation episodes
    eval_envs: 4  # Number of parallel environments for evaluation
    video_log_interval: 1  # Log videos every evaluation call
  
  # Training settings
  log_interval: 1  # Log every rollout

# ====================
# Named Configs
# ====================

# Tiger Door Key with probabilistic dropout
gymnasium_tigerdoorkey: &tigerdoorkey_base
  task: gymnasium_TigerDoorKey-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b', cnn_keys: '^$'}
  dropout:
    schedule_type: 'constant'
    base_probability: 0.3  # Higher chance of keeping keys (70% inclusion rate)
  # Evaluation keys for subset comparison (same as original PPO dropout)
  eval_keys:
    env1:
      mlp_keys: '\b(neighbors|door|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'
    env2: # must press button to reveal tiger/treasure up-down relative position. starts with key
      mlp_keys: '\b(neighbors_unprivileged_nokey|door_unprivileged|doors_unlocked|position|has_key)\b'
      cnn_keys: '^$'
    env3:  # must get key to reveal unlocked doors. starts with button
      mlp_keys: '\b(neighbors_unprivileged_nobutton|door|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'
    env4:  # must press button and get key to reveal unlocked doors and tiger/treasure relative position. starts with nothing
      mlp_keys: '\b(neighbors_unprivileged|door_unprivileged|doors_unlocked_unprivileged|position|has_key)\b'
      cnn_keys: '^$'
  # Reduced learning rate for stability in simple grid environment
  ppo:
    learning_rate: 1e-4
  exp_name: "tigerdoorkey_ppo_dropout_any"

gymnasium_tigerdoorkeylarge:
  << : *tigerdoorkey_base
  task: "gymnasium_tigerdoorkeylarge"
  total_timesteps: 500000
  exp_name: "tigerdoorkeylarge_ppo_dropout_any"

gymnasium_maze: &maze_base
  task: gymnasium_Maze7x7-v0
  total_timesteps: 400000
  keys: {mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b', cnn_keys: '^$'}
  dropout:
    schedule_type: 'linear'
    start_probability: 0.7  # Start with higher dropout
    end_probability: 0.2    # End with lower dropout
    total_episodes: 5000    # Reduce dropout over 5000 episodes
  # Evaluation keys for subset comparison (same as original PPO dropout)
  eval_keys:
    env1:  # Goal position, neighbors 5x5
      mlp_keys: '\b(goal_position|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    env2:  # Goal position, neighbors_3x3
      mlp_keys: '\b(goal_position|position|neighbors_3x3)\b'
      cnn_keys: '^$'
    env3:  # No goal position, neighbors_3x3
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3)\b'
      cnn_keys: '^$'
    env4:  # No goal position, neighbors_5x5
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3|neighbors_5x5)\b'
      cnn_keys: '^$'
    env5: # No goal position, neighbors_3x3 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged)\b'
      cnn_keys: '^$'
    env6: # No goal position, neighbors_5x5 unprivileged
      mlp_keys: '\b(goal_position_unprivileged|position|neighbors_3x3_unprivileged|neighbors_5x5_unprivileged)\b'
      cnn_keys: '^$'
  exp_name: "maze_ppo_dropout_any"

gymnasium_maze11:
  << : *maze_base
  task: gymnasium_Maze11x11-v0
  total_timesteps: 1500000
  dropout:
    schedule_type: 'linear'
    start_probability: 0.7
    end_probability: 0.2
    total_episodes: 10000  # Longer schedule for more complex environment
  exp_name: "maze11_ppo_dropout_any"

# Blind Pick environment with exponential dropout schedule
gymnasium_blindpick: &blindpick_base
  task: gymnasium_FOFixedGripper2DBlind7cmPick-v0
  total_timesteps: 5000000
  num_envs: 32
  log_keys_video: [camera_front]
  keys: {mlp_keys: '.*', cnn_keys: '.*'} # robot_state, touch, camera_front, camera_side, gripper_camera_rgb
  dropout:
    schedule_type: 'exponential'
    start_probability: 0.6   # Start with moderate dropout
    decay_rate: 0.9995      # Slow decay
    min_probability: 0.1    # Maintain some dropout throughout
  # Evaluation keys for subset comparison (same as original PPO dropout)
  eval_keys:
    env1: # full privileged observations -- reach directly for object
      mlp_keys: '.*'
      cnn_keys: '.*'
    env2:  # NO TOUCH -- reach directly without touch
      mlp_keys: '\b(robot_state|obj_state)\b'
      cnn_keys: '.*'
    env3:  # NO OBJ STATE -- reach directly using image and not obj state
      mlp_keys: '\b(robot_state|touch)\b'
      cnn_keys: '.*'
    env4: # NO OBJ STATE, only a wrist camera -- searching behavior
      mlp_keys: '\b(robot_state|touch)\b'
      cnn_keys: 'gripper_camera_rgb'
    env5:  # full state, only wrist camera -- searching but obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'gripper_camera_rgb'
    env6:  # full state, only front camera -- searching but image, obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'camera_front'
    env7:  # full state, only side camera -- searching but image, obj_state gives it away
      mlp_keys: '.*'
      cnn_keys: 'camera_side'
  exp_name: "blindpick_ppo_dropout_any"

# Blind Pick with different dropout schedules for experimentation
blindpick_constant_dropout:
  << : *blindpick_base
  dropout:
    schedule_type: 'constant'
    base_probability: 0.4  # 60% inclusion rate
  exp_name: "blindpick_constant_dropout"

blindpick_linear_dropout:
  << : *blindpick_base
  dropout:
    schedule_type: 'linear'
    start_probability: 0.8  # High dropout initially
    end_probability: 0.2   # Low dropout later
    total_episodes: 15000  # Long schedule for complex task
  exp_name: "blindpick_linear_dropout"

# Blind Pick with Oracle State (simplified observations)
blindpick_oracle_state:
  << : *blindpick_base
  keys: {mlp_keys: r'\b(robot_state|obj_state)\b', cnn_keys: '^$'}
  dropout:
    schedule_type: 'constant'
    base_probability: 0.5  # 50% dropout for simplified observations
  exp_name: "blindpick_oracle_state_dropout_any" 